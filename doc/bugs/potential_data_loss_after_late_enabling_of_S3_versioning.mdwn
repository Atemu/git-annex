If a S3 remote is set up with exporttree=yes, and some files are stored on
it, and then it's later changed to also have versioning=yes, an exporttree
that removes some of the original files can lose the only remaining copy of
them.

exporttree does not currently check numcopies before removing from an
export. Normally all export remotes are untrusted, so they can't count as a
copy, and so removing something from them cannot violate numcopies.

An appendonly remote, such as S3 with exporttree=yes, is supposed to not
let git-annex remove content from it. So such a remote can be not
untrusted, and exporttree can remove content from its exported tree without
violating numcopies since the content is still supposed to be available in
the remote.

The S3 remote that gets versioning=yes enabled *after* some content has
been stored on it without versioning violates the requirements for an
appendonly remote. When exporttree removes a file from that S3 remote,
it could have contained the only copy of the file, and it may not have
versioning info for that file, so the only copy is lost.

So are those requirements wrong, or is the S3 remote wrong? In either case,
something needs to be done to prevent this situation from losing data.

# change S3

S3 remotes could refuse to allow versioning=yes to be set during
enableremote, and only allow it at initremote time. And check that the
bucket does indeed have versioning enabled or refuse to allow that
configuration. That would avoid the problem.

(Unless the user changed the bucket configuration later to not allow
versioning. But if they did so, and an old version of the bucket was the
only place a file was stored, they would lose data without git-annex being
run at all, so it's equivilant to them deleting the bucket, so this seems
not something it needs to worry about).

There is [an yet-unmerged pull
request](https://github.com/aristidb/aws/pull/255) to let buckets be
created with versioning enabled, that is kind of a prerequisite for this
change, otherwise the user would need to manually make the bucket and
enable versioning before initremote.

So, plan:

* Wait for the PutBucketVersioning pull request to be merged.
* Add a remote.name.s3-versioning-enabled which needs to be true
  in order for exporttree to remove files from a versioned remote.
* Enable versioning and set remote.name.s3-versioning-enabled during initremote,
  when versioning=yes. If aws library is too old to enable versioning,
  initremote should fail.
* Do no allow changing versioning= during enableremote.
* Any repos that used versioning=yes before this point will see removal
  of files from them fail. The user can either manually set
  remote.name.s3-versioning-enabled (if they are sure they enabled it from
  the beginning), or can disable versioning. (Or perhaps other resolutions
  to the problem, up to the user.)

# change exporttree

Exporttree could do some kind of check, but the regular numcopies drop check
doesn't seem right for this situation. 

The regular pre-drop check with an untrusted export remote would fail when
a file's content was removed from the local repo (or was overwritten with
annex.thin eg), even though the export remote, being untrusted, can't be
guaranteed to continue to have a copy of the file. It would add some
measure of safety, but not a strong guarantee of not losing the last copy,
in that situation. To continue with the export, the user would need to
first somehow download the object from from the export remote. The UI for
that is lacking.

The regular pre-drop check with a versioned S3 export remote is just wrong,
because the content is not normally going to be removed from that remote,
due to the versioning.

Hmm, the versioned S3 remote could check, in removeExport 
that the keys being removed have S3 versioning information available. If
not, refuse to remove, or treat this removal as a drop, verifying numcopies
first.
(Luckily S3 does not implement removeExportDirectory so don't need to handle
that much harder case.)

Need to consider export split brain situations. If repo A has updated a
file in the S3 export, and has not yet synced to repo B, and repo B is
removing the file, it would check the wrong key.

Also, repo A and repo B may both run storeExport using different keys, 
and one overwrite the other, which cannot be detected by either until they
merge. It's always possible to lose an object this way, if no S3 version ID was
recorded.

This approach is not looking promising..
