[[!comment format=mdwn
 username="joey"
 subject="""comment 2"""
 date="2018-08-09T20:18:46Z"
 content="""
One of v6's big problems is that dropping or getting an annexed file
updates the file in the working tree, which makes git status think
the file is modified, even though the clean filter will output
the same pointer as before. Runing `git add` to clear it up is quite
expensive since the large file content has to be read.
Maybe a long-running filter process could avoid this problem.

----

If git can be coaxed somehow into re-running the smudge filter,
git-annex could provide the new worktree content to git via it,
and let git update the working tree.

Git would make a copy, which git-annex currently does, so the only
added overhead would be sending the file content to git down the pipe.
(Well and it won't use reflink for the copy on COW filesystems.)

annex.thin is a complication, but it could be handled by hard linking the
work tree file that git writes back into the annex, overwriting the file that
was there. (This approach could also make git checkout of a branch honor 
annex.thin.)

How to make git re-run the smudge filter? It needs to want to update the
working tree. One way is to touch the worktree files and then run 
`git checkout`. Although this risks losing modifications the user made
to the files so would need to be done with care.

That seems like it would defer working tree updates until the git-annex
get command was done processing all files. Sometimes I want to use a
file while the same get command is still running for other files.
It might work to use the "delay" capability of the filter process
interface. Get git to re-smudge all affected files, and when it
asks for content for each, send "delayed". Then as git-annex gets
each file, respond to git's "list_available_blobs" with a single blob,
which git should request and use to update the working tree.
"""]]
